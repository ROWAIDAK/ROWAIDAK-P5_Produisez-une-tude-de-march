#!/usr/bin/env python
# coding: utf-8

# # Produisez une √©tude de march√©
# **Sc√©nario**
# # Mission 1: Construisez l'√©chantillon contenant l'ensemble des pays disponibles.
#    1. La population par pays
#    2. Donn√©es sur les bilans alimentaires mondiaux (2019)
#    3. Les valeurs de PIB par habitant,
#    4. Production viande de Volailles, taux d'autosuffisanc
#    5. Dataframe principal   
# # Mission 2 : r√©alisation d'un dendrogramme
# 1. Environnement
# 2. Aper√ßu des corr√©lations
# 3. Classification des pays via Clustering Hi√©rarchique Ascendant (CHA)
# 4. Attribution des 5 groupes et World map de r√©partition
# 5. Centro√Ødes des clusters
# 6. Description et critique des clusters
# 
# # Mission 3 : Analyse en Composantes Principales (ACP)
# 1. Application de l'algorithme du K-Means
# 2. Visualisation des clusters en ACP pour la projection des donn√©es
# 3. ACP - Cercle des corr√©lations¬∂
# 4. S√©lections des pays sur groupes Kmeans.
# # Mission 4 : Tests statistiques
# 3. Test d'ad√©quation de Kolmogorov-Smirno
# 2. Tester l'√©galit√© de la variance
# 
# Le projet est sur github
# 
# https://github.com/ROWAIDAK/ROWAIDAK-P5_Produisez-une-tude-de-march

# # Sc√©nario
# Votre entreprise **d'agroalimentaire** souhaite se d√©velopper √† l'international. Elle est sp√©cialis√©e dans...
#  le poulet !
# 
# L'international, oui, mais pour l'instant, le champ des possibles est bien large : aucun pays particulier ni aucun continent n'est pour le moment choisi. Tous les pays sont envisageables !
# 
# Votre objectif sera d'aider **√† cibler plus particuli√®rement certains pays**, dans le but d'approfondir ensuite l'√©tude de march√©. Plus particuli√®rement, l'id√©al serait de produire des "groupes" de pays, plus ou moins gros, dont on conna√Æt les caract√©ristiques.
# 
# Dans un premier temps, la strat√©gie est plut√¥t d'exporter les produits plut√¥t que de produire sur place, c'est-√†-dire dans le(s) nouveau(x) pays cibl√©(s).

# 
# Pour identifier les pays propices √† une insertion dans le march√© du poulet,
#  Il vous a √©t√© demand√© de cibler les pays. 
#  Etudier les r√©gimes alimentaires de chaque pays, notamment en termes de prot√©ines d'origine animale et en termes de calories.
# 
# 
# Construisez votre √©chantillon contenant l'ensemble des pays disponibles, chacun caract√©ris√© par ces variables :
# 
# diff√©rence de population entre une ann√©e ant√©rieure (au choix) et l'ann√©e courante, exprim√©e en pourcentage ;
# proportion de prot√©ines d'origine animale par rapport √† la quantit√© totale de prot√©ines dans la disponibilit√© alimentaire du pays ;
# disponibilit√© alimentaire en prot√©ines par habitant ;
# disponibilit√© alimentaire en calories par habitant.
# 
# Construisez un dendrogramme contenant l'ensemble des pays √©tudi√©s, puis coupez-le afin d'obtenir 5 groupes.
# 
# Caract√©risez chacun de ces groupes selon les variables cit√©s pr√©c√©demment, et facultativement selon d'autres variables que vous jugerez pertinentes (ex : le PIB par habitant). Vous pouvez le faire en calculant la position des centro√Ødes de chacun des groupes, puis en les commentant et en les critiquant au vu de vos objectifs.
# 
# Donnez une courte liste de pays √† cibler, en pr√©sentant leurs caract√©ristiques. Un d√©coupage plus pr√©cis qu'en 5 groupes peut si besoin √™tre effectu√© pour cibler un nombre raisonnable de pays. 
# 
# Visualisez vos  partitions dans le premier plan factoriel obtenu par ACP.
# 
# Dans votre partition, vous avez obtenu des groupes distincts. V√©rifiez donc qu'ils diff√®rent r√©ellement. Pour cela, r√©alisez les tests statistiques suivants :
# 
# un test d'ad√©quation : parmi les 4 variables, ou parmi d'autres variables que vous trouverez pertinentes, trouvez une variable dont la loi est normale ;
# un test de comparaison de deux populations (dans le cas gaussien) : choisissez 2 clusters parmi ceux que vous aurez d√©termin√©. Sur ces 2 clusters, testez la variable gaussienne gr√¢ce √† un test de comparaison.

# In[374]:


# Librairies
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import seaborn as sns
import sklearn as sk
import scipy as sp
import pca as pca
from sklearn import cluster
from sklearn.cluster import KMeans


# In[375]:


#Versions utilis√©es
print("Jupyter Notebook : " + str(pd.__version__))
print("Pandas : " + str(pd.__version__))
print("Numpy : " + str(np.__version__))
print("Seaborn : " + str(sns.__version__))

# Styles Seaborn
sns.set( 
    style='whitegrid',
    context='notebook',
    palette='Paired',
    rc={'figure.figsize':(8,5)})


# # Mission 1:  Construisez l'√©chantillon contenant l'ensemble des pays disponibles.
# 

# ## 1. La population par pays
# la variation de la population entre 2009 et 2019, en %, qui sera positive en cas de croissance ou n√©gative en cas de baisse d√©mographique ;

# Sources FAO (https://www.fao.org/faostat/fr/#data/OA)

# In[376]:


df_population = pd.read_csv('datas/fao-populations_2019- 2009.csv' , header=0, sep=",", decimal=".")
df_population.head(2)


# In[377]:


df_population = df_population[['Code zone (FAO)', 'Zone', 'Ann√©e', 'Valeur']]
df_population['Valeur'] = df_population['Valeur']*1000
#pivot
df_population = df_population.pivot_table(index=['Code zone (FAO)','Zone'], columns='Ann√©e', values = 'Valeur', aggfunc = sum).reset_index()
#rename
df_population.columns = ['Code zone (FAO)','Zone', '2009', '2019']

#variable creation
df_population['√âvolution population (%)'] = round((df_population['2019'] - df_population['2009']) /
                                               df_population['2019'] *100,2)

df_population= df_population.rename(columns= {'2019': 'population',})
df_population= df_population[[ 'Code zone (FAO)','Zone'  ,'population' ,'√âvolution population (%)']]

#show

df_population.head(2)


# ## 2. Donn√©es sur les bilans alimentaires mondiaux (2019)
# 

# 
# Sources FAO (http://www.fao.org/faostat/fr/#data)
# 

# In[378]:


df_dispo_alimentaire = pd.read_csv('datas/les bilans alimentaires 2019 2.csv', header=0, sep=",", decimal=".")
#Il y a un probl√®me avec le site Web de la FAO traitant de la langue fran√ßaise lors du t√©l√©chargement du fichier
df_dispo_alimentaire.head(3)


# In[379]:


df_dispo_alimentaire = df_dispo_alimentaire.pivot_table(index=[ 'Zone','Code zone (FAO)'],
                                columns=['?l?ment', 'Produit'],
                                values = 'Valeur',
                                aggfunc=sum).reset_index()
df_dispo_alimentaire.head()


# In[380]:


df_dispo_alimentaire["ratio_proteines_animales(%)"] = round((df_dispo_alimentaire[('Disponibilit? de prot?ines en quantit? (g/personne/jour)', 'Produits Animaux')]
                                                                         /df_dispo_alimentaire[('Disponibilit? de prot?ines en quantit? (g/personne/jour)', 'Total General')])*100,2)
df_dispo_alimentaire.head(2)


# In[381]:


df= pd.merge(df_population, df_dispo_alimentaire, on="Code zone (FAO)")

df.head()


# In[382]:


df = df[['Code zone (FAO)','Zone', 'population','√âvolution population (%)',
         ('Disponibilit? alimentaire (Kcal/personne/jour)', 'Total General'),
         ('Disponibilit? de prot?ines en quantit? (g/personne/jour)', 'Total General'),
         ('ratio_proteines_animales(%)','') ]]
df.head()


# In[383]:


df.rename(columns={('Disponibilit? alimentaire (Kcal/personne/jour)', 'Total General'):'dispo_calories',
                              ('Disponibilit? de prot?ines en quantit? (g/personne/jour)', 'Total General'):'dispo_proteines',
                            ('ratio_proteines_animales(%)','' ):'ratio_prot_anim'}
                     ,inplace=True)

df['dispo_calories'] = df['dispo_calories']*365
df['dispo_proteines'] = df['dispo_proteines']*365
df.head()


# ## 3.  Les valeurs de PIB par habitant,
# https://www.fao.org/faostat/fr/#data/FS

# In[384]:


# Dataframes des donn√©es compl√©mentaires
# Indicateurs Macro (PIB et croissance)
df_PIB_habitant = pd.read_csv('datas/PIB.csv', header=0, sep=',', decimal='.')

df_PIB_habitant.head(2)


# In[385]:


df_PIB_habitant = df_PIB_habitant[['Code zone (FAO)','√âl√©ment','Valeur','Produit']]
                        

df_PIB_habitant = df_PIB_habitant.pivot_table(index=['Code zone (FAO)'], columns='√âl√©ment', values='Valeur', aggfunc=sum).reset_index()
df_PIB_habitant = df_PIB_habitant.rename(columns={'Valeur US $ par habitant':"PIB_par_habitant" })
df_PIB_habitant = df_PIB_habitant[['Code zone (FAO)','PIB_par_habitant']]

df_PIB_habitant.head(2)


# ## 4. Production viande de Volailles, taux d'autosuffisance
# d√©fini en √©conomie comme le rapport entre les importations et la disponibilit√© int√©rieure du pays ;
# 
# https://www.fao.org/faostat/fr/#data/FBS

# In[386]:


df_viande_volailles = pd.read_csv('datas/la viande de volailles1!.csv', header=0, sep=",", decimal=".")
df_viande_volailles.head(2)


# In[387]:


df_viande_volailles = df_viande_volailles.pivot_table(index=['Code zone (FAO)'],
                                columns=['√âl√©ment'],
                                values = 'Valeur',
                                aggfunc=sum).reset_index()
#Le taux_suffisance= Production  √∑ (Production alimentaire domestique + importations „Éº exportations) √ó100
df_viande_volailles['taux_suffisance']=((df_viande_volailles['Production'] ) / 
                                           (df_viande_volailles['Disponibilit√© int√©rieure']))*100



#le taux de d√©pendance aux importations, d√©fini en √©conomie comme le rapport entre les importations et la disponibilit√© int√©rieure du pays ;

df_viande_volailles['dep_import']=(df_viande_volailles['Importations - Quantit√©'] /
                                                        df_viande_volailles['Disponibilit√© int√©rieure'])*100


#Le taux d'auto-suffisance alimentaire est un indice permettant de mesurer l'importance de la production alimentaire d'un pays par rapport √† sa consommation int√©rieure.
df_viande_volailles = df_viande_volailles[['Code zone (FAO)','taux_suffisance','dep_import']]

df_viande_volailles.head(2)


# **Le taux d'auto-suffisance alimentaire** est un indice permettant de mesurer l'importance de la production alimentaire d'un pays par rapport √† sa consommation int√©rieure.
# 
# 
# 

# # Dataframe principal

# In[388]:


df= pd.merge(df, df_PIB_habitant, on="Code zone (FAO)")


df= pd.merge(df, df_viande_volailles, on="Code zone (FAO)")
df.head(2)


# In[389]:


df.info()


# In[390]:


#Trouver les valeurs manquantes
df_null=  df.loc[df.isnull().any(axis=1)]
df_null


# In[391]:


df=df.dropna()


# In[392]:


df.describe()


# In[393]:


#Retrait du pays 'France' sur notre √©chantillon car nous n'exportons pas vers notre pays. 

df = df[df['Zone'] != 'France']


# In[394]:


#Calcul de fr√©quence en Chine
df[df['Zone'].str.contains('hin')]


# In[395]:


#Supprimer la ligne Chine  car elle est en double. 
df = df[df['Zone'] != 'Chine']
### Suppression de la cor√©e du Nord
df = df[df['Zone'] != 'R√©publique populaire d√©mocratique de Cor√©e']


# In[396]:


#V√©rification d'√©ventuelles valeurs manquantes et/ou en doubles dans l'√©chantillon
print(df.duplicated().sum())
print(df.isna().sum())


# In[397]:


### Suppression des petits pays
df = df[df.population >= 500000]


# In[398]:


df= df.rename(columns={'Zone': 'pays'} )


# In[399]:


df.style.background_gradient(cmap='BrBG')


# In[400]:


df.to_csv('exports/df.csv', index=False)
df.shape


# # Mission 2 : r√©alisation d'un dendrogramme
# 
# 
# Construisez un dendrogramme contenant l'ensemble des pays √©tudi√©s, puis coupez-le afin d'obtenir 5 groupes.
# 
# Caract√©risez chacun de ces groupes selon les variables cit√©s pr√©c√©demment, et facultativement selon d'autres variables que vous jugerez pertinentes (ex : le PIB par habitant). Vous pouvez le faire en calculant la position des centro√Ødes de chacun des groupes, puis en les commentant et en les critiquant au vu de vos objectifs.
# 
# Donnez une courte liste de pays √† cibler, en pr√©sentant leurs caract√©ristiques. Un d√©coupage plus pr√©cis qu'en 5 groupes peut si besoin √™tre effectu√© pour cibler un nombre raisonnable de pays. 
# 

# ## Environnement
# 

# In[401]:


#pays comme index
df_clus=df.set_index('pays', drop=True, append=False, inplace=False, verify_integrity=False)

# pr√©paration des donn√©es pour le clustering
df_clus = df_clus[["√âvolution population (%)", "dispo_calories",
                                 "dispo_proteines",'ratio_prot_anim',
                                 'PIB_par_habitant','taux_suffisance','dep_import']]

df_clus.head(2)


# ## Aper√ßu des corr√©lations

# In[402]:


plt.figure(figsize=(15,5))

mask = np.zeros_like(df_clus.corr())
mask[np.triu_indices_from(mask)] = True

sns.heatmap(df_clus.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')

plt.xticks(rotation=25, ha='right')
plt.title('La corr√©lation des variables',  fontsize=18, pad=20)
plt.savefig('exports/La corr√©lation des variables.')
plt.show()


# #### Observations
# on peut v√©rifier la corr√©lation des variables avec une matrice des corr√©lations. 
# On peut constater une corr√©lation positive forte entre  la dispo en prot√©ines et la dispo en calories‚ÄØ, le ratio de prot√©ines animales, le PIB.
# Dans une moindre mesure, on retrouve √©galement une corr√©lation n√©gative entre le pourcentage d'√©volution de la population et les diff√©rents r√©gimes alimentaires des pays. 
# On note une relation n√©gative entre l'autosuffisance et le pourcentage de d√©pendance aux importations
# 
# **Nous sommes int√©ress√©s par les pays les plus susceptibles de consommer du poulet, donc ceux ayant un fort ratio de prot√©ines animales. Les corr√©lations montrent que ces pays sont susceptibles d'avoir un fort PIB, et de grandes disponibilit√©s en prot√©ines et calories. 
# Nous nous int√©ressons √©galement aux pays d√©pendants des importations qui ne sont pas autosuffisants.**

# ## Classification des pays via Clustering Hi√©rarchique Ascendant (CHA)
# 
# La classification sera √©tablie sur la base des variables suivantes :
# 
# **La diff√©rence de population entre l'ann√©e 2013 et l'ann√©e 2019, exprim√©e en pourcentage;\
# La proportion de prot√©ines d'origine animale par rapport √† la quantit√© totale de prot√©ines dans la disponibilit√© alimentaire du pays;\
# La disponibilit√© alimentaire en grammes de prot√©ines par habitant ;\
# La disponibilit√© alimentaire en Kcal par habitant.\
# PIB par habitant.\
# Rapport de d√©pendance √† l'importation.\
# La taux suffisance(Ÿ™).**
# 
# L'√©chantillon comporte peu de variables sur la dimension dite du "R√©gime alimentaire" et et variables √©conomiques, il comporte √©galement un nombre de pays "ma√Ætrisables" qui permet de commencer par une classification hi√©rarchique. Algorithme qui a une forte complexit√© algorithmique en temps et en espace, le clustering hi√©rarchique est recommand√© pour les petits √©chantillons.
# 
# 
# 
# Le clustering permet de regrouper des individus similaires, c'est-√†-dire qu'il va partitionner l'ensemble des individus. On cherche donc √† ce que les groupes soient :
# 
#     **Resserr√©s sur eux-m√™mes : deux points qui sont proches devraient appartenir au m√™me groupe.
#   
#     **Loin les uns des autres, c‚Äôest-√†-dire qu‚Äôils soient fortement diff√©renci√©s.
# 
# Au pr√©alable, il est n√©cessaire de centrer-r√©duire les donn√©es. C‚Äôest √† dire, recalculer chaque valeur de mani√®re √† ce que la moyenne de la variable soit √©gale √† 0 et la variance et l‚Äô√©cart-type √©galent 1. Pour une variable donn√©e, on soustrait √† chaque valeur la moyenne de la variable, puis on divise le tout par l‚Äô√©cart-type.
# 
# Ensuite, nous pouvons proc√©der √† la classification ascendante hi√©rarchique selon la m√©thode de Ward. 
# 

# In[403]:



from sklearn import preprocessing
from scipy.cluster.hierarchy import linkage, fcluster, dendrogram


# dans array numpy
X = df_clus.values 
#nous allons extraire les donn√©es d'expression de 153 pays  dans un tableau de donn√©es num√©riques .
 #X comporte uniquement les donn√©es  , il ne contient pas les √©tiquettes des √©chantillons.

pays = df_clus.index
 #Conservons les √©tiquettes de chaque √©chantillon  dans la variable pays.

#centering and reduction
#il est n√©cessaire de centrer-r√©duire les donn√©es. C‚Äôest √† dire, recalculer chaque valeur de mani√®re √† ce que 
#la moyenne de la variable soit √©gale √† 0 et la variance et l‚Äô√©cart-type √©galent 1. Pour une variable donn√©e,
#on soustrait √† chaque valeur la moyenne de la variable, puis on divise le tout par l‚Äô√©cart-type.


std_scale = preprocessing.StandardScaler().fit(X)
X_scaled = std_scale.transform(X)
#Standardisez les caract√©ristiques en supprimant la moyenne et  en divisant par l'√©cart type

#X_scaled =array([[-0.90652015,  0.30068128,  0.61600031, ..., -0.47817227,
        #-1.15900796,  0.84483273],
       #[ 1.29355645, -1.35075836, -1.15709131, ..., -0.68784567,
        #-0.7198978 ,  0.54941119],...,

        

# Clustering hi√©rarchique avec librairie scipy
z=linkage(X_scaled, method='ward' ,metric='euclidean')
#euclidien pour mesurer la distance entre les observations au sein d'une classe
#ward pour mesurer la distance entre les rang√©es

# Affichage du dendogramme
plt.figure(figsize=(12,25), dpi=300)
plt.title('Dendogramme de classification ascendante hi√©rarchique (CAH)')
plt.xlabel('distance')
dendrogram(
    z,#Regroupement hi√©rarchique encod√© avec la matrice renvoy√©e par la fonction de linkage.
    labels = pays,
    orientation = "right",
    color_threshold=7
)
plt.savefig('exports/dendogram_CAH_1.png')
plt.show()


# Une fois le dendrogramme obtenu, nous pouvons choisir en combien de clusters nous pouvons diviser notre arbre. Ici, une partition en 5 clusters permet d‚Äôavoir des groupes de taille raisonnable √† analyser.
# 
# Graphiquement, on voit bien que la m√©thode de Ward a permis de minimiser les distances intra-classes √† chaque regroupement.
# 
# 
# Essayons de caract√©riser chacun de ces clusters en regardant la valeur de leurs centro√Øde pour chaque variable. Les centro√Ødes repr√©sentent tout simplement la valeur moyenne d‚Äôune variable pour un cluster donn√©.
# Diff√©rences entre les clusters :

# In[404]:


plt.figure(figsize=(12,8), dpi=300)
plt.title('Dendogramme de classification ascendante hi√©rarchique tronqu√©')
plt.xlabel('distance')
plt.grid(False)
dendrogram(
    z,      #linkage(X_cr, method='ward' ,metric='euclidean'),
    truncate_mode='lastp', #Les p derniers clusters non singleton form√©s dans la liaison sont les seuls n≈ìuds non feuilles dans la liaison¬†; 
    p = 5,
    labels = pays,
    orientation = "right",
    show_contracted=True,)
plt.savefig('exports/dendogram_CAH_truncated_1.png')
plt.show()


# ## Attribution des 5 groupes et World map de r√©partition

# In[405]:


# Coupage du dendrogramme en 5 clusters avec Scipy
groupe_cah = fcluster(z, 5, criterion='maxclust')
#array([2, 1, 4, 5, 2, 2, 5, 3, 3, 2, 4,....
#fcluster: formez des clusters plats √† partir du clustering hi√©rarchique d√©fini par la matrice de liaison donn√©e.

#affichage des pays et leurs groupes
df_groupage_1 = pd.DataFrame(columns=["groupe_cah","pays"])
df_groupage_1["pays"] = df_clus.index
df_groupage_1["groupe_cah"] = groupe_cah

df_groupage_1


# In[406]:


# Jointure pour ajout des groupes dans le dataframe principal
df_groupes_cah = pd.merge(df_clus, df_groupage_1, on="pays")
df_groupes_cah.to_csv('exports/groupes_cah.csv', index=False)
df_groupes_cah.sample(5)


# ## Centro√Ødes des clusters

# In[410]:


#Premi√®re comparaison des moyennes afin d'identifier le groupe de pays le plus porteur √† ce niveau de l'analyse
centro√Ødes_CAH=df_groupes_cah.groupby('groupe_cah').mean()
centro√Ødes_CAH.to_csv('exports/P5 centro√Ødes CAH.csv', index=False)
centro√Ødes_CAH


# In[285]:


#Pr√©paration de sous-ensembles permettant de caract√©riser les groupes un √† un
df_groupe1_cah = df_groupes_cah[df_groupes_cah['groupe_cah'] == 1]
df_groupe2_cah = df_groupes_cah[df_groupes_cah['groupe_cah'] == 2]
df_groupe3_cah = df_groupes_cah[df_groupes_cah['groupe_cah'] == 3]
df_groupe4_cah = df_groupes_cah[df_groupes_cah['groupe_cah'] == 4]
df_groupe5_cah = df_groupes_cah[df_groupes_cah['groupe_cah'] == 5]


# In[286]:




#Pays du groupe 1 et 2 identifi√©s comme potentiellement int√©ressants

print('groupe 1')
print('----------')
print(df_groupe1_cah['pays'].unique())
print('------------------------------------------------------------')
print('------------------------------------------------------------')


print('groupe 2')
print('----------')
print(df_groupe2_cah['pays'].unique())
print('------------------------------------------------------------')
print('------------------------------------------------------------')


print('groupe 3')
print('----------')
print(df_groupe3_cah['pays'].unique())
print('------------------------------------------------------------')
print('------------------------------------------------------------')


print('groupe 4')
print('----------')
print(df_groupe4_cah['pays'].unique())
print('------------------------------------------------------------')
print('------------------------------------------------------------')


print('groupe 5')
print('----------')
print(df_groupe5_cah['pays'].unique())
print('------------------------------------------------------------')
print('------------------------------------------------------------')





# ## Description et critique des clusters

# In[287]:




#Comparaison visuelle des groupes par Boxplot, en abscisse les num√©ros des groupes
plt.figure(figsize=(20, 20))
sns.set(style="whitegrid")

plt.subplot(221)
sns.boxplot(data=df_groupes_cah, x='groupe_cah', y='dispo_calories')


plt.subplot(222)
sns.boxplot(data=df_groupes_cah, x='groupe_cah', y='dispo_proteines')

plt.subplot(223)
sns.boxplot(data=df_groupes_cah, x='groupe_cah', y='ratio_prot_anim')

plt.subplot(224)
sns.boxplot(data=df_groupes_cah, x='groupe_cah', y='√âvolution population (%)')


plt.savefig("exports/boxplot_dendogramme3.png")




plt.show(block=False)


# In[288]:


#Comparaison visuelle des groupes par Boxplot, en abscisse les num√©ros des groupes
plt.figure(figsize=(20, 20))
sns.set(style="whitegrid")

plt.subplot(221)
sns.boxplot(data=df_groupes_cah, x='groupe_cah', y='PIB_par_habitant')


plt.subplot(222)
sns.boxplot(data=df_groupes_cah, x='groupe_cah', y='dep_import')


plt.subplot(223)
sns.boxplot(data=df_groupes_cah, x='groupe_cah', y='taux_suffisance')


plt.savefig("exports/boxplot_dendogramme3.png")




plt.show(block=False)


# Description et critique des clusters 
# 
# Les groupes 1 et 2, situ√©s principalement en Afrique, ont la d√©mographie la plus forte et le PIB par habitant le plus bas. 
# 
# Les groupes 4 et 5, au contraire, refl√®tent des pays plus riches en moyenne, comme les √âtats-Unis et la plupart des pays europ√©ens. 
# 
# Fait int√©ressant, le groupe 5 a un bon PIB et une forte croissance d√©mographique.
# 
# En termes de disponibilit√© alimentaire, ces tendances suivent les m√™mes tendances que le PIB. Les pays les plus pauvres ont moins acc√®s √† la nourriture. 
# 
# La consommation de prot√©ines animales confirme le partage, avec une forte alimentation carn√©e de la part des groupes 3,4 et 5 
# On voit que les groupes 2 et 5 importent beaucoup plus de viande de volaille qu'ils n'en produisent. 
# 
# En fait, ils ont les taux d'autosuffisance les plus bas. Au contraire, les groupes 2, 3 et 4 sont raisonnablement autosuffisants. 
# 
#  
# Compte tenu de tous ces crit√®res, quel groupe serait apte √† √™tre s√©lectionn√© comme candidat pour notre march√© international‚ÄØ? 
# 
#  Je pense que les groupes qui d√©pendent fortement des importations sont les meilleures cibles pour nos ventes.
# Nous choisirons le groupe 5 car il est fortement d√©pendant des importations, a un taux de croissance d√©mographique √©lev√© et un PIB par habitant √©lev√©.
# 
# 
# Nous choisissons √©galement le groupe 4, car le PIB par habitant est beaucoup plus √©lev√© dans ces pays, ce qui nous permettra de vendre notre production plus facilement et √† un meilleur prix. Et le plus √©lev√© en termes de consommation de prot√©ines animales et de calories. Il comprend √©galement des pays g√©ographiquement proches de la France.
#  

# In[289]:


df_select_pays_cah = df_groupes_cah[df_groupes_cah["groupe_cah"].isin([4,5]) == True]

df_select_pays_cah.to_csv('exports/S√©lections des pays sur CAH.csv', index=False)
df_select_pays_cah.shape
df_select_pays_cah


# In[290]:


df_select_pays_cah.shape


# En conclusion pour cette premi√®re section, 26 pays sont susceptibles de devenir une cible appropri√©e pour l'entreprise. La demande sera pr√©sente dans ces pays. Appliquons une autre m√©thode, la m√©thode K-Means, afin de pouvoir comparer cette premi√®re s√©lection.

# # Mission 3 : Analyse en Composantes Principales (ACP)
# 

# 
# Le clustering K-Means est une m√©thode de clustering simple mais puissante qui cr√©e ùëò segments distincts des donn√©es o√π la variation au sein des clusters est aussi petite que possible. Pour trouver le nombre optimal de clusters, je vais essayer diff√©rentes valeurs de ùëò et calculer l'inertie, ou score de distorsion, pour chaque mod√®le.
#  L'inertie mesure la similarit√© du cluster en calculant la distance totale entre les points de donn√©es et leur centre de cluster le plus proche. Les clusters avec des observations similaires ont tendance √† avoir des distances plus petites entre eux et un score de distorsion plus faible dans l'ensemble.
# 
# ## La m√©thode du coude nous aidera √† d√©terminer le nombre de groupes.
# 
# ‚Ä¢Nous choisissons 'K' manuellement, par visualisation.
# 
# ‚Ä¢ Calculer les distances entre les points d'un cluster (With-in Cluster Sum of Squares "WCSS").
# 
# ‚Ä¢ Si nous minimisons 'WCSS', nous avons atteint la solution de clustering parfaite.
# 

# In[291]:


from sklearn.cluster import KMeans
from sklearn import cluster


K=range(1,10)
k_means = []
#On fait une boucle de 1 √† 10 pour tester toutes ces possibili√©ts
for k in K:
    #pour chaque k, on cr√©e un mod√®le et on l‚Äôajuste
    km=KMeans(n_clusters=k,init="k-means++").fit(X_scaled)
     #on stocke l‚Äôinertie associ√©e
    k_means.append(km.inertia_)


#Visualisation des valeurs d'inertie pour chaque nombre de cluster
plt.plot(range(1, 10), k_means, marker='o')
plt.show()


# In[292]:


mycenters = pd.DataFrame({'groupe_km' : K, 'WSS' : k_means})
mycenters


# On remarque que le nombre de 5 Clusters n'est pas id√©al pour le Kmeans. La meilleure alternative serait 2 Clusters. Si l'on veut partitionner un peu plus, il faudrait consid√©rer un K = 3 ou 4.
# 
# il est conseill√© de choisir k = 5 .
#  Un clustering  en 5 permettra de de comparer le partitionnement avec les groupes de la classification hi√©rarchique. Il est pertinent de comparer les deux m√©thodes sur le m√™me nombre de clusters.
# 
# 

# In[293]:



#Clustering K-Means en 5 clusters
km = cluster.KMeans(n_clusters=5)
km.fit(X_scaled)
#R√©cup√©ration des clusters attribu√©s √† chaque individu (classes d'appartenance)
clusters_km = km.labels_
clusters_km


# # Visualisation des clusters en ACP pour la projection des donn√©es
# 

# Le principe de **la r√©duction de dimension** est de r√©duire la complexit√© superflue d'un dataset en projetant ses donn√©es dans un espace de plus petite dimension .
# 
# Le principe  de **ACP** est de projeter nos donn√©es sur des axes appel√©s Composantes Principales, en cherchant √† minimiser la distance entre nos points et leur projections. De cette mani√®re on r√©duit la dimension pr√©servant au maximum la variance de nos donn√©es. Pour **Pr√©server un maximum de variance pour optenir la projection qu'il soit la plus fid√®le possible √† nos donn√©es.**
#  
#  Analyse Pour trouver les axes de projection (xp): 
#  Pour faire √ßa dans point de vue math√©matique on
#  1. On calcule la matrice de covariance des donn√©es 
#  2. On d√©termine les vecteurs propres de cette matrice : ce sont les Composantes Principales 
#  3. (On projette les donn√©es sur ces axes)
#  
# 
# 
#  
# **L'ACP (Analyse en Composante Principale) permettra une visualisation des clusters pays sur le premier plan factoriel (ou plus). Il deviendra alors facile de pouvoir appr√©hender le "comportement" des diff√©rents groupes.**

# PCA est un transformer ! 
# 1. D√©finir le nombre de composantes 
# 2. Transformer les donn√©es avec fit transform()
# 
# Il y a deux cas possibles pour choisir le nombre de composantes sur lesquels projeter nos donn√©es? et bien :
# 1. Celui dans lequel vous cherchez √† visualiser vos donn√©es dans un espace de 2d ou 3D ,pour √ßa c'est tr√®s simple, le nombre de composants doit √™tre √©gale √† deux ou trois
# 2. Celui dans lequel vous cherchez √† compresser vos donn√©es pour acc√©l√©rer l'apprentissage de la machine sur des taches de classification ou de r√©gression, pour √ßa il faut choisir le nombre de composantes de telle sorte √† pr√©server entre 95 et 99 % de la variance de vos donn√©es.
# 
# 
# L‚Äôenjeu d‚Äôune ACP est de trouver le meilleur plan de projection ayant la plus grande inertie, c‚Äôest √† dire limitant le plus la perte d‚Äôinformation originelle. Les 7 variables seront synth√©tis√©es en de nouvelles variables : PC1, PC2, etc...
# 
# Comme pr√©c√©demment, une ACP ne peut se faire que si les donn√©es sont centr√©es et r√©duites (transformation pour que moyenne = 0, √©cart-type = 1).
# 
# ### D√©finir le nombre de composantes 

# In[294]:


import pca as pca
from sklearn import decomposition

#decomposition.PCA: R√©duction de la dimensionnalit√© lin√©aire √† l'aide de la d√©composition en valeurs singuli√®res des donn√©es pour les projeter dans un espace de dimension inf√©rieure.
pca = decomposition.PCA().fit(X_scaled) #sklearn
X_projected = pca.transform(X_scaled)


#nous allons examiner quel est le pourcentage de variance pr√©server pour chacune de nos composantes.
scree = pca.explained_variance_ratio_*100      #Le param√®tre pca.explained_variance_ratio_ renvoie un vecteur de la variance expliqu√©e par chaque dimension.
#array([49.88630268, 24.51850599, 10.68333729,  5.72429344,  4.27437791,3.66079486,  1.25238783])

plt.bar(np.arange(len(scree))+1, scree)
plt.plot(np.arange(len(scree))+1, scree.cumsum(),c="red",marker='o')   
plt.xlabel("rang de l'axe d'inertie")
plt.ylabel("pourcentage d'inertie")
plt.title("Eboulis des valeurs propres")
plt.savefig("exports/Eboulis des valeurs propres.png")

plt.show()

#Pourcentage de variance expliqu√©e par les composantes principales √† l'aide de .explained_variance_ratio_
print(scree.cumsum())




# In[295]:


scree 


# Environ 75 % de la variance des donn√©es s'explique par ces deux premi√®res composantes.
# La m√©thode du coude pr√©cise une forte repr√©sentation de nos variables sur les deux premi√®res composantes principales, le premier axe factoriel.
# 

# # ACP - Cercle des corr√©lations

# In[296]:


def cerle_corr(pcs, n_comp, pca, axis_ranks, labels=None, label_rotation=0, lims=None):
    for d1, d2 in axis_ranks:
        if d2 < n_comp:

            # initialisation de la figure
            #fig, ax = plt.subplots(figsize=(12,(n_comp*2)))
            #ax.set_aspect('equal', adjustable='box')
            fig=plt.figure(figsize=(12,12))
            fig.subplots_adjust(left=0.1,right=0.9,bottom=0.1,top=0.9)
            ax=fig.add_subplot(111)
            ax.set_aspect('equal', adjustable='box')

            # d√©termination des limites du graphique
            if lims is not None :
                xmin, xmax, ymin, ymax = lims
            else :
                xmin, xmax, ymin, ymax = -1, 1, -1, 1
        
            # affichage des fl√®ches
            plt.quiver(np.zeros(pcs.shape[1]), np.zeros(pcs.shape[1]),
                   pcs[d1,:], pcs[d2,:], 
                   angles='xy', scale_units='xy', scale=1, color="grey")
            
            # affichage des noms des variables  
            if labels is not None:  
                for i,(x, y) in enumerate(pcs[[d1,d2]].T):
                    if x >= xmin and x <= xmax and y >= ymin and y <= ymax :
                        plt.text(x, y, labels[i], fontsize='14', ha='center', va='center', rotation=label_rotation, color="blue", alpha=0.5)
            
            # affichage du cercle
            circle = plt.Circle((0,0), 1, facecolor='none', edgecolor='b')
            plt.gca().add_artist(circle)

            # d√©finition des limites du graphique
            plt.xlim(xmin, xmax)
            plt.ylim(ymin, ymax)
        
            # affichage des lignes horizontales et verticales
            plt.plot([-1, 1], [0, 0], color='grey', ls='--')
            plt.plot([0, 0], [-1, 1], color='grey', ls='--')

            # nom des axes, avec le pourcentage d'inertie expliqu√©
            plt.xlabel('F{} ({}%)'.format(d1+1, round(100*pca.explained_variance_ratio_[d1],1)))
            plt.ylabel('F{} ({}%)'.format(d2+1, round(100*pca.explained_variance_ratio_[d2],1)))

            plt.title("Cercle des corr√©lations (F{} et F{})".format(d1+1, d2+1))
            plt.savefig("exports/Cercle des corr√©lations (F1 et F2).png")

            plt.show(block=False)


# In[297]:


pcs = pca.components_
cerle_corr(pcs, 4, pca, [(0,1)], labels = np.array(df_clus.columns))


# Dans notre √©tude, le premier plan factoriel de l‚ÄôACP a permis de conserver 75 % de l‚Äôinformation totale.
# 
# Ici, l‚Äôaxe PC2 refl√®te bien le taux de d√©pendance aux importations et le taux d‚Äôautosuffisance. Plus la valeur de l‚Äôaxe 2 est positive, et plus le pays est importateur. Au contraire, plus les valeurs sont n√©gatives, et plus le pays est autosuffisant avec sa production de viande de volailles et importe peu. 
# 
# De m√™me, l'axe PC1 est une combinaison de la disponibilit√© totale de prot√©ines et de calories, du PIB, de la proportion de prot√©ines animales et de l'√©volution de la population. 
# 
# Plus la forte croissance d√©mographique d'un pays est importante, plus sa valeur positive sur l'axe PC1 est √©lev√©e. 
# 
# √Ä l'inverse, plus la valeur de l'axe PC1 est n√©gative, plus le PIB du pays est √©lev√©, plus la consommation de prot√©ines animales et la disponibilit√© alimentaire de prot√©ines et de calories sont √©lev√©es. 
# 
# La prot√©ine  est la variable qui contribue le plus √† l'axe PC1. 
# Enfin, il existe une certaine relation entre les variables du PIB, la proportion de prot√©ines animales et la disponibilit√© de prot√©ines totales et de calories.  
# 
#  

# In[298]:


#pca = decomposition.PCA().fit(X_scaled) #sklearn
#X_projected = pca.transform(X_scaled)


#Coordonn√©es factorielles 
plt.figure(figsize=(20,20))
plt.subplot(122)
plt.scatter(X_projected[:, 0], X_projected[:, 1], c=km.labels_)
plt.xlabel('F{} ({}%)'.format(1, round(100*scree[0],1)))
plt.ylabel('F{} ({}%)'.format(2, round(100*scree[1],1)))
plt.title("Projection en 5 clusters des {} individus sur le 1er plan factoriel".format(X_projected.shape[0]))

plt.savefig("exports/projection_clusters.png")
plt.show()


# On peut d'ailleurs calculer les valeurs de ces variables synth√©tiques F1 et F2 qui pourraient remplacer les autres variables :
# 
# 

# In[299]:


X_projected


# In[300]:


#Calcul des composantes principales
#Ici seulement F1 et F2 seront utiles √† l'interpr√©tation attendue
X_projected = pca.transform(X_scaled)

df_facto = pd.DataFrame(X_projected, index=df_clus.index, columns=["F" + str(i+1) for i in range(7)]).iloc[:, :2]
df_facto.head() #Affichage des 5 premi√®res lignes


# On obtient donc un tableau de 5 lignes et 7 colonnes, pourquoi ?
# Nous avons cinq groupes et sept variables.
# **Pour r√©duire les dimensions, nous avons besoin de pca**

# L'analyse sera plus fine en 5 clusters. De plus, la comparaison sera possible avec les 5 groupes identifi√©s lors du pr√©c√©dent partitionnement, le contexte nous oriente davantage vers un clustering en 5 partitions.
# 
# Maintenant, il est n√©cessaire de caract√©riser chacun de ces groupes selon nos 8 variables. La position des centro√Ødes de chacun des groupes indiquera le ou les meilleurs clusters. C'est l'avantage de proc√©der en K-Means, afin d'obtenir directement des valeurs centr√©es et r√©duites, facilitant l'analyse. ‚¨áÔ∏è

# In[301]:


df_groupes_cah['groupe_km'] = clusters_km
gb = df_groupes_cah.groupby('groupe_km')
nk = gb.size()
print(nk)


# In[302]:


# Moyennes conditionnelles
mk = gb.mean()
mk


# In[303]:


# Ajout des variables synth√©tiques F1 et F2
df_boxkm = pd.merge(df_groupes_cah, df_facto, on="pays", how="left")
df_boxkm = df_boxkm.sort_values("groupe_km")
df_boxkm.head()


# In[414]:


#les centro√Ødes des groupes et leurs coordonn√©es dans chacune des dimensions.
les_centro√Ødes_groupes= df_boxkm.groupby('groupe_km').mean()
les_centro√Ødes_groupes.to_csv('exports/P5_centro√Ødes_groupe_km.csv', index=False)

les_centro√Ødes_groupes


# In[305]:


def boxplot_cluster_km(var):
    data_boxplot = []
    groupes_pays = df_boxkm["groupe_km"].unique()
    
    for groupe in groupes_pays :
        subset = df_boxkm[df_boxkm.groupe_km == groupe]
        data_boxplot.append(subset[var])

    fig, ax1 = plt.subplots(figsize=(12, 8))
    fig.subplots_adjust(left=0.075, right=0.95, top=0.9, bottom=0.25)

    bp = ax1.boxplot(data_boxplot, notch=0, vert=1, whis=1.5, labels=["0", "1", "2", "3", "4"])

    ax1.yaxis.grid(True, linestyle='-', which='major', color='lightgrey',
               alpha=0.4)
    ax1.set_axisbelow(True)
    ax1.set_title(("Distribution de " + var +" par Cluster Kmeans"), fontsize=22)
    ax1.set_ylabel(var)
    ax1.set_xlabel("Cluster")
    ax1.set_xlim(0, len(data_boxplot) + 0.5)

    plt.show()


# In[306]:


boxplot_cluster_km('√âvolution population (%)')


# In[307]:


boxplot_cluster_km('dispo_calories')


# In[308]:


boxplot_cluster_km('dispo_proteines')


# In[309]:


boxplot_cluster_km('ratio_prot_anim')


# In[310]:


boxplot_cluster_km('PIB_par_habitant')


# In[311]:


boxplot_cluster_km('taux_suffisance')


# In[312]:


boxplot_cluster_km('dep_import')


# In[313]:


boxplot_cluster_km('F1')


# In[314]:


boxplot_cluster_km('F2')


# # S√©lections des pays sur groupes Kmeans.
# 
# Bas√© sur les m√™mes crit√®res qui ont √©t√© suivis lors de la s√©lection des pays par CAH (les groupes qui d√©pendent fortement des importations).
#   et le PIB par habitant est beaucoup plus √©lev√©), 
# Les groupes de Kmeans √† conserver sont ceux o√π F2 est sup√©rieur √† 1, soit les groupes 2, 3 et 4.
# Nous constatons que les groupes 3 et 4 atteignent un niveau √©lev√© de consommation de prot√©ines animales et ont un PNB √©lev√©,

# In[315]:


select_clusters_kmeans = df_boxkm.groupby("groupe_km").mean().reset_index()
select_clusters_kmeans = df_boxkm[df_boxkm["F1"]<-1]["groupe_km"].unique()
select_clusters_kmeans


# In[329]:


select_clusters_kmeans = df_boxkm.groupby("groupe_km").mean().reset_index()
select_clusters_kmeans = df_boxkm[df_boxkm["F2"]>1]["groupe_km"].unique()
select_clusters_kmeans


# In[338]:



df_select_pays_kmeans = df_boxkm[df_boxkm["groupe_km"].isin([1,4]) == True]
df_select_pays_kmeans.to_csv('exports/Des groupe pays sur kmeans.csv', index=False)

df_select_pays_kmeans


# In[331]:


df_select_pays_kmeans.shape


# Ces groupes comptent 35 pays 
# 
#  Le Kmeans  nous a permis de choisir un meilleur pays. Ce sera notre groupe cible de deuxi√®me niveau

# In[343]:


df_compare_cah_kmeans = pd.merge(df_select_pays_cah[['pays','groupe_cah']], df_select_pays_kmeans[['pays','groupe_km']],
                     on='pays', how='outer')
df_compare_cah_kmeans.to_csv('exports/Des groupe pays sur kmeans et CAH.csv', index=False)

df_compare_cah_kmeans


# # Liste des pays et recommandations

# In[340]:


df_select_pays_km_cah=df_compare_1.dropna()
df_select_pays_km_cah


# In[337]:


df_select_pays_km_cah.shape


# 
#   Dans le premier niveau, on va choisir les pays qui sont communs aux groupes du Kmeans  et CAH, on obtient 20 pays.
# 
# Liste des pays et recommandations Dans un premier temps, on sugg√®re donc de cibler les pays de l'UE, pour leur proximit√© et la facilit√© des √©changes commerciaux : l'Allemagne, le Danemark, la Su√®de ,Luxembourg et le Norv√®ge. 
# 
# 
# Prudence avec le Royaume-Uni, puisqu'avec le Brexit, les √©changes commerciaux avec l'UE sont actuellement compliqu√©s. Pour autant, consid√©rant les int√©r√™ts √©conomiques mutuels, les r√©centes directives qui ont √©t√© prises pour favoriser ces √©changes sont en notre faveur. 
# 
#  
# 
# De plus, il n'est pas pr√©f√©rable d'exporter vers l'Am√©rique car c'est l'un des pays les plus exportateurs de poulet au monde en plus du Br√©sil. 
# 
#  
# 
# Dans un second temps, on pourrait √©galement cibler les pays comme Hong Kong, le Japon, √âmirats arabes unis, ainsi que le Kowe√Øt. 
# 
#  
# 
# FAO - Poultry production - March√©s et commerce https://www.fao.org/poultry-production-products/aspects-socio-economiques/marches-et-commerce/fr/ 
# 
#  
# 
# "Le Br√©sil est le principal exportateur de viande de volaille, suivi par les √âtats-Unis et les Pays-Bas. Les principaux pays importateurs sont la Chine, le Japon, le Mexique et le Royaume-Uni." 
# 
#  
# 
# "Les pays les moins avanc√©s sont de plus en plus d√©pendants des importations de viande de volaille. Le niveau de leurs importations est pass√© de 3 pour cent en 1961 √† environ 30 pour cent en 2013." 
# 
#  
# 
#  
# 
#  
# 
# https://www.fao.org/poultry-production-products/production/fr/ 
# 
#  
# 
# "Les √âtats-Unis d'Am√©rique sont le plus grand producteur de viande de volaille √† l‚Äô√©chelle de la plan√®te: ils produisent en effet 17 pour cent de la production mondiale. Viennent ensuite la Chine et le Br√©sil." 
# 
#  
# 
# "Pour r√©pondre √† la demande croissante, la production de viande de volaille mondiale a bondi, passant de 9 √† 132 millions de tonnes entre 1961 et 2019." 
# 
#  
# 
# "En 2019, la viande de volaille repr√©sentait environ 39 pour cent de la production mondiale de viande." 
# 
#  
# 
# "Dans les pays en d√©veloppement, environ 80 pour cent des m√©nages ruraux √©l√®vent des volailles." 

# # Tests statistiques
# 
# 
# Dans notre partition, nous avons des groupes s√©par√©s. Pour v√©rifier qu'ils diff√®rent r√©ellement. Nous avons effectu√© les test statistique  Kolmogorov-Smirnov qui est un test d'ad√©quation : parmi les sept variables, nous recherchons une variable avec une distribution normale.
# On peut tester l‚Äôad√©quation de la 'Disponibilit√© alimentaire de prot (g/personne/jour)' √† une loi normale .
# 
# 
# 
# Pour √©valuer si cet √©chantillon peut √™tre consid√©r√© comme gaussien, on peut √©tudier l'√©cart entre la fonction de r√©partition d'une loi normale et celle estim√©e de notre √©chantillon : la fonction de r√©partition empirique !
# Plus cette quantit√© est grande, plus on est enclin √† rejeter l'hypoth√®se comme quoi l'√©chantillon est gaussien.
# 

# In[158]:


import scipy.stats as st
from scipy import stats
from scipy.stats import ks_2samp


# ### Test d'ad√©quation de Kolmogorov-Smirnov :

# ### V√©rification des hypoth√®ses

# **H0 = La variable suit donc  une loi normale .**
# 
# **H1 = La variable ne suit pas une loi normale.**

# In[159]:


df_groupes_cah.head(1)


# In[160]:


#Kolmogorov Smirnov test
stat, p= st.ks_2samp(df_groupes_cah['dispo_proteines'], 
            np.random.normal(df_groupes_cah['dispo_proteines'].mean(), 
                             df_groupes_cah['dispo_proteines'].std(ddof=0),
                             df_groupes_cah['dispo_proteines'].count()))

print('Statistics=%.3f, p=%.3f' % (stat, p))

#Interpr√©tation
alpha = 0.05
if p > alpha:
    print(' Nous pouvons accepter H0 pour des niveaux de test de 5¬†%')
else:
    print('H0 est rejet√©e √† un niveau de test de 5%')
    


# ### Test d'ad√©quation de Shapiro-Wilk :
# Le test sera doubl√© par celui de Shapiro-Wilk.
# Nous avons √©chantillonn√© les deux groupes de pays s√©lectionn√©s aux CAH 4 et 5
# Comme nous avions un petit √©chantillon on peux faire Le test  Shapiro-Wilk.
# Le test de Shapiro-Wilk est un test statistique utilis√© pour v√©rifier si une variable continue suit une distribution normale. L'hypoth√®se nulle (H0) indique que la variable est normalement distribu√©e, et l'hypoth√®se alternative (H1) indique que la variable n'est PAS normalement distribu√©e. Donc apr√®s avoir ex√©cut√© ce test :
# 
# Si p ‚â§ 0,05¬†: alors l'hypoth√®se nulle peut √™tre rejet√©e (c'est-√†-dire que la variable n'est PAS distribu√©e normalement).
# Si p > 0,05¬†: alors l'hypoth√®se nulle ne peut pas √™tre rejet√©e (c'est-√†-dire que la variable PEUT √äTRE normalement distribu√©e).
# 

# In[161]:


#creation of the df with only clusters 4 & 5
c4c5 = df_groupes_cah[(df_groupes_cah['groupe_cah'] == 4) | (df_groupes_cah['groupe_cah'] == 5)]


# In[163]:


#normality of variables in c4c5
import pingouin as pg
pg.normality(c4c5, method='shapiro', alpha=0.05).drop('groupe_cah')

#normality: test de normalit√© univari√©.


# Nous constatons que les variables sont dispo_calories, dispo_proteines, ratio_proteines_animales(%), et PIB_par habitant. distribu√© selon la loi normale.

# In[164]:


#histogram
sns.histplot(data=c4c5, x='dispo_proteines', kde=True, color='#4cb2ff')
plt.savefig("exports/tester la normalit√© dispo_proteines.jpg")

plt.show()


# #### Disponibilit√© alimentaire √©nerg√©tique
# 

# In[165]:


#histogram
sns.histplot(data=c4c5, x='dispo_calories', kde=True, color='#4cb2ff')
plt.savefig("exports/tester la normalit√© dispo_calories.jpg")
plt.show()


# #### Importation de viande de volaille
# 

# In[166]:


#histogram
sns.histplot(data=c4c5, x='√âvolution population (%)', kde=True, color='#4cb2ff')
plt.savefig("exports/tester la normalit√© √âvolution populations.jpg")

plt.show()


# #### Pourcentage de prot√©ine animale
# 

# In[167]:


#histogram
sns.histplot(data=c4c5, x='ratio_prot_anim', kde=True, color='#4cb2ff')
plt.savefig("exports/tester la normalit√© ratio_pro.jpg")

plt.show()


# #### Produit Int√©rieur Brut
# 

# In[168]:


#histogram
sns.histplot(data=c4c5, x='PIB_par_habitant', kde=True, color='#4cb2ff')
plt.savefig("exports/tester la normalit√© PIB.jpg")

plt.show()


# #### Importation de viande de volaille
# 

# In[169]:


#histogram
sns.histplot(data=c4c5, x='taux_suffisance', kde=True, color='#4cb2ff')
plt.savefig("exports/tester la normalit√© taux_suffisance.jpg")

plt.show()


# In[170]:


#histogram
sns.histplot(data=c4c5, x='dep_import', kde=True, color='#4cb2ff')
plt.savefig("exports/tester la normalit√© taux_d√©pendance_importations.jpg")

plt.show()


# ### Test de comparaison de deux clusters dans le cas gaussien.
# 
# Si on souhaite comparer deux √©chantillons (i.i.d) gaussiens, il nous suffit en fait de comparer leurs param√®tres : leur moyenne Œº1 et Œº2, et leur variance œÉ21 et œÉ22.
# La m√©thodologie la plus classique est d'effectuer de mani√®re s√©quentielle :
# 
# Un test d'√©galit√© des variances.
# 
# Un test d'√©galit√© des moyennes.
# 
# Si les variances ne sont pas consid√©r√©es comme √©gales, les deux √©chantillons n'ont pas la m√™me loi. Si les variances sont consid√©r√©es comme √©gales, il est alors possible d'estimer cette variance sur les deux √©chantillons √† la fois, et de tester l'√©galit√© des moyennes en utilisant cette variance empirique globale.
# Notons qu'il est n√©anmoins possible d'effectuer un test de comparaison des moyennes sous hypoth√®se de variances diff√©rentes. Il ne s'agit pas d'une comparaison des lois, mais alors d'une comparaison simple des moyennes.
# 
# 

# ## La variable 'dispo_proteines' suit une loi normale et sera par cons√©quent choisie pour le test.
# 
# 

# In[171]:


#boxplot
color = sns.color_palette('pastel')

plt.figure(figsize=(30,15)) 

sns.boxplot(data=c4c5[(c4c5["groupe_cah"]== 4) | (c4c5["groupe_cah"]== 5)], x='dispo_proteines', y='groupe_cah', orient='h', palette=color,
            fliersize=4 , showfliers=True, showmeans=True, meanprops={"marker":"o", 
                                                                      "markerfacecolor":"white",
                                                                      "markeredgecolor":"white", 
                                                                      "markersize":"10"})

plt.xlabel('Dispo alim en prot√©ine', fontsize=20, labelpad=30, fontweight='bold')
plt.xticks(fontsize=20)
plt.ylabel('Cluster', fontsize=20, labelpad=30, fontweight='bold')
plt.yticks(fontsize=20)
plt.title('Disponibilit√© alimentaire en prot√©ine pour les cluster 4 et 5', fontsize=35, pad=50)
plt.savefig("exports/Disponibilit√© alimentaire en prot√©ine pour les cluster 4 et 5.jpg")


plt.show()


# ### Tester l'√©galit√© de la variance
# 
# **H0 = Les variance sont √©gales .**
# 
# **H1 = Les variance ne sont pas √©gales.** 

# In[172]:


#On teste tout d‚Äôabord l‚Äô√©galit√© des variances √† l‚Äôaide de la commande
pg.homoscedasticity(df_groupes_cah, dv='dispo_proteines',  group='groupe_cah', method='levene', 
                    alpha=0.05)
#Thomoscedasticity :tester l'√©galit√© de la variance.


# ### Tester l'√©galit√© des moyennes
# 
# **H0 = Les moyennes sont √©gales .**
# 
# **H1 = Les moyennes ne sont pas √©gales.** 

# In[173]:


#On teste ensuite l‚Äô√©galit√© des moyennes √† l‚Äôaide de la commande
pg.ttest(df_groupes_cah['dispo_proteines'][df_groupes_cah['groupe_cah'] == 4],
         df_groupes_cah['dispo_proteines'][df_groupes_cah['groupe_cah'] == 5],
         paired=False,
        
         confidence=0.95)


# In[174]:


Œ± = 0.05

if Œ± > pg.ttest(df_groupes_cah['dispo_proteines'][df_groupes_cah['groupe_cah'] == 4],
             df_groupes_cah['dispo_proteines'][df_groupes_cah['groupe_cah'] == 5]).iloc[0,3] : 
    
    print("La p-value √©tant inf√©rieure au risque Œ±, on rejette donc H0, les moyennes des deux groupes sont diff√©rentes.")
else :
    print("La p-value √©tant sup√©rieur au risque Œ±, H0 est donc vrai, les moyennes des deux groupes sont √©gales.")


# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:




